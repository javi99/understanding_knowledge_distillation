{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom PIL import Image, ImageEnhance, ImageOps\nimport PIL\nimport torch.optim as optim\nfrom torch.autograd import Variable\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-09T15:35:00.776539Z","iopub.execute_input":"2023-06-09T15:35:00.777058Z","iopub.status.idle":"2023-06-09T15:35:00.795947Z","shell.execute_reply.started":"2023-06-09T15:35:00.777012Z","shell.execute_reply":"2023-06-09T15:35:00.787108Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"!mkdir best_models\n!mkdir checkpoints","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:35:00.811707Z","iopub.execute_input":"2023-06-09T15:35:00.812032Z","iopub.status.idle":"2023-06-09T15:35:03.672592Z","shell.execute_reply.started":"2023-06-09T15:35:00.812003Z","shell.execute_reply":"2023-06-09T15:35:03.671375Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"mkdir: cannot create directory ‘best_models’: File exists\nmkdir: cannot create directory ‘checkpoints’: File exists\n","output_type":"stream"}]},{"cell_type":"code","source":"# def find_lr(train_loader,criterion_val,init_value = 1e-8, final_value=10., beta = 0.98):\n#     num = len(train_loader)-1\n#     mult = (final_value / init_value) ** (1/num)\n#     lr = init_value\n#     optimizer.param_groups[0]['lr'] = lr\n#     avg_loss = 0.\n#     best_loss = 0.\n#     batch_num = 0\n#     losses = []\n#     log_lrs = []\n#     for data in train_loader:\n#         batch_num += 1\n#         #As before, get the loss for this mini-batch of inputs/outputs\n#         inputs,labels = data\n#         inputs, labels = Variable(inputs), Variable(labels)\n#         optimizer.zero_grad()\n#         outputs = student(inputs)\n#         loss = criterion_val(outputs, labels)\n#         #Compute the smoothed loss\n#         avg_loss = beta * avg_loss + (1-beta) *loss.data[0]\n#         smoothed_loss = avg_loss / (1 - beta**batch_num)\n#         #Stop if the loss is exploding\n#         if batch_num > 1 and smoothed_loss > 4 * best_loss:\n#             return log_lrs, losses\n#         #Record the best loss\n#         if smoothed_loss < best_loss or batch_num==1:\n#             best_loss = smoothed_loss\n#         #Store the values\n#         losses.append(smoothed_loss)\n#         log_lrs.append(math.log10(lr))\n#         #Do the SGD step\n#         loss.backward()\n#         optimizer.step()\n#         #Update the lr for the next step\n#         lr *= mult\n#         optimizer.param_groups[0]['lr'] = lr\n#     return log_lrs, losses\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:35:03.675647Z","iopub.execute_input":"2023-06-09T15:35:03.676047Z","iopub.status.idle":"2023-06-09T15:35:03.682222Z","shell.execute_reply.started":"2023-06-09T15:35:03.676007Z","shell.execute_reply":"2023-06-09T15:35:03.681151Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport os\nimport numpy as np\nimport random\nimport shutil\nimport time\nimport warnings\nimport random\nfrom enum import Enum\nfrom torch.nn import functional as F\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.datasets as datasets\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.optim.lr_scheduler import StepLR, CosineAnnealingWarmRestarts\nfrom torch.utils.data import Subset\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision\n# from cifar_mixup import CIFAR100\n# from split_val_test import get_train_valid_loader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nimport logging\n# logging.basicConfig()\n# logging.root.setLevel(logging.NOTSET)\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\nlogging.debug(\"test\")\n\n\n\nbest_acc1 = 0\n\n\ndef get_train_valid_loader(data_dir,\n                           batch_size,\n                           transform_list,\n                           random_seed,\n                           valid_size=0.1,\n                           shuffle=True,\n                           show_sample=False,\n                           num_workers=4,\n                           pin_memory=False):\n    \"\"\"\n    Utility function for loading and returning train and valid\n    multi-process iterators over the CIFAR-10 dataset. A sample\n    9x9 grid of the images can be optionally displayed.\n    If using CUDA, num_workers should be set to 1 and pin_memory to True.\n    Params\n    ------\n    - data_dir: path directory to the dataset.\n    - batch_size: how many samples per batch to load.\n    - augment: whether to apply the data augmentation scheme\n      mentioned in the paper. Only applied on the train split.\n    - random_seed: fix seed for reproducibility.\n    - valid_size: percentage split of the training set used for\n      the validation set. Should be a float in the range [0, 1].\n    - shuffle: whether to shuffle the train/validation indices.\n    - show_sample: plot 9x9 sample grid of the dataset.\n    - num_workers: number of subprocesses to use when loading the dataset.\n    - pin_memory: whether to copy tensors into CUDA pinned memory. Set it to\n      True if using GPU.\n    Returns\n    -------\n    - train_loader: training set iterator.\n    - valid_loader: validation set iterator.\n    \"\"\"\n    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n\n    \n    # load the dataset\n    test_dataset = torchvision.datasets.CIFAR100(\n        root=data_dir, train=False,\n        download=True, transform=transform_list,\n    )\n\n    valid_dataset = torchvision.datasets.CIFAR100(\n        root=data_dir, train=False,\n        download=True, transform=transform_list,\n    )\n\n    num_test = len(test_dataset)\n    indices = list(range(num_test))\n    split = int(np.floor(valid_size * num_test))\n\n    if shuffle:\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n\n    train_idx, valid_idx = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_idx)\n    valid_sampler = SubsetRandomSampler(valid_idx)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, sampler=train_sampler,\n        num_workers=num_workers, pin_memory=pin_memory,\n    )\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset, batch_size=batch_size, sampler=valid_sampler,\n        num_workers=num_workers, pin_memory=pin_memory,\n    )\n    return test_loader, valid_loader\n\nclass EarlyStopper:\n    def __init__(self, patience=1, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.min_validation_loss = np.inf\n\n    def early_stop(self, validation_loss):\n        if validation_loss < self.min_validation_loss:\n            self.min_validation_loss = validation_loss\n            self.counter = 0\n        elif validation_loss > (self.min_validation_loss + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n\nclass CIFAR100Policy(object):\n    \"\"\" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n        Example:\n        >>> policy = CIFAR100Policy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     CIFAR10Policy(),\n        >>>     transforms.ToTensor()])\n    \"\"\"\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.1, \"invert\", 7, 0.2, \"contrast\", 6, fillcolor),\n            SubPolicy(0.7, \"rotate\", 2, 0.3, \"translateX\", 9, fillcolor),\n            SubPolicy(0.8, \"sharpness\", 1, 0.9, \"sharpness\", 3, fillcolor),\n            SubPolicy(0.5, \"shearY\", 8, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.5, \"autocontrast\", 8, 0.9, \"equalize\", 2, fillcolor),\n\n            SubPolicy(0.2, \"shearY\", 7, 0.3, \"posterize\", 7, fillcolor),\n            SubPolicy(0.4, \"color\", 3, 0.6, \"brightness\", 7, fillcolor),\n            SubPolicy(0.3, \"sharpness\", 9, 0.7, \"brightness\", 9, fillcolor),\n            SubPolicy(0.6, \"equalize\", 5, 0.5, \"equalize\", 1, fillcolor),\n            SubPolicy(0.6, \"contrast\", 7, 0.6, \"sharpness\", 5, fillcolor),\n\n            SubPolicy(0.7, \"color\", 7, 0.5, \"translateX\", 8, fillcolor),\n            SubPolicy(0.3, \"equalize\", 7, 0.4, \"autocontrast\", 8, fillcolor),\n            SubPolicy(0.4, \"translateY\", 3, 0.2, \"sharpness\", 6, fillcolor),\n            SubPolicy(0.9, \"brightness\", 6, 0.2, \"color\", 8, fillcolor),\n            SubPolicy(0.5, \"solarize\", 2, 0.0, \"invert\", 3, fillcolor),\n\n            SubPolicy(0.2, \"equalize\", 0, 0.6, \"autocontrast\", 0, fillcolor),\n            SubPolicy(0.2, \"equalize\", 8, 0.8, \"equalize\", 4, fillcolor),\n            SubPolicy(0.9, \"color\", 9, 0.6, \"equalize\", 6, fillcolor),\n            SubPolicy(0.8, \"autocontrast\", 4, 0.2, \"solarize\", 8, fillcolor),\n            SubPolicy(0.1, \"brightness\", 3, 0.7, \"color\", 0, fillcolor),\n\n            SubPolicy(0.4, \"solarize\", 5, 0.9, \"autocontrast\", 3, fillcolor),\n            SubPolicy(0.9, \"translateY\", 9, 0.7, \"translateY\", 9, fillcolor),\n            SubPolicy(0.9, \"autocontrast\", 2, 0.8, \"solarize\", 3, fillcolor),\n            SubPolicy(0.8, \"equalize\", 8, 0.1, \"invert\", 3, fillcolor),\n            SubPolicy(0.7, \"translateY\", 9, 0.9, \"autocontrast\", 1, fillcolor)\n        ]\n\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return \"AutoAugment CIFAR100 Policy\"\n\n    \nclass SubPolicy(object):\n    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n        ranges = {\n            \"shearX\": np.linspace(0, 0.3, 10),\n            \"shearY\": np.linspace(0, 0.3, 10),\n            \"translateX\": np.linspace(0, 150 / 331, 10),\n            \"translateY\": np.linspace(0, 150 / 331, 10),\n            \"rotate\": np.linspace(0, 30, 10),\n            \"color\": np.linspace(0.0, 0.9, 10),\n            \"posterize\": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n            \"solarize\": np.linspace(256, 0, 10),\n            \"contrast\": np.linspace(0.0, 0.9, 10),\n            \"sharpness\": np.linspace(0.0, 0.9, 10),\n            \"brightness\": np.linspace(0.0, 0.9, 10),\n            \"autocontrast\": [0] * 10,\n            \"equalize\": [0] * 10,\n            \"invert\": [0] * 10\n        }\n\n        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n        def rotate_with_fill(img, magnitude):\n            rot = img.convert(\"RGBA\").rotate(magnitude)\n            return Image.composite(rot, Image.new(\"RGBA\", rot.size, (128,) * 4), rot).convert(img.mode)\n\n        func = {\n            \"shearX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"shearY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            \"translateX\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n                fillcolor=fillcolor),\n            \"translateY\": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n                fillcolor=fillcolor),\n            \"rotate\": lambda img, magnitude: rotate_with_fill(img, magnitude),\n            # \"rotate\": lambda img, magnitude: img.rotate(magnitude * random.choice([-1, 1])),\n            \"color\": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n            \"posterize\": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n            \"solarize\": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n            \"contrast\": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"sharpness\": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"brightness\": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            \"autocontrast\": lambda img, magnitude: ImageOps.autocontrast(img),\n            \"equalize\": lambda img, magnitude: ImageOps.equalize(img),\n            \"invert\": lambda img, magnitude: ImageOps.invert(img)\n        }\n\n        # self.name = \"{}_{:.2f}_and_{}_{:.2f}\".format(\n        #     operation1, ranges[operation1][magnitude_idx1],\n        #     operation2, ranges[operation2][magnitude_idx2])\n        self.p1 = p1\n        self.operation1 = func[operation1]\n        self.magnitude1 = ranges[operation1][magnitude_idx1]\n        self.p2 = p2\n        self.operation2 = func[operation2]\n        self.magnitude2 = ranges[operation2][magnitude_idx2]\n\n\n    def __call__(self, img):\n        if random.random() < self.p1: img = self.operation1(img, self.magnitude1)\n        if random.random() < self.p2: img = self.operation2(img, self.magnitude2)\n        return img\n    \n    \ndef main(args):\n    global best_acc1\n    global device\n\n    writer = SummaryWriter(log_dir=os.path.join(\"tensorboard\", args.experiment_name))\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        print(\"Use GPU for training : CUDA version {}, device:{}\".format(torch.version.cuda,device))\n\n    # create model\n#     teacher=args.teacher\n    student=args.student\n    print(\"=> Loading Student model '{}'\".format(student._get_name()))\n#     logging.info(\"=> Loading Teacher model '{}'\".format(teacher._get_name()))\n        \n\n    if not torch.cuda.is_available():\n        print('using CPU, this will be slow')\n\n#     teacher = teacher.to(device)\n    student = student.to(device)\n\n    # define loss function (criterion), optimizer, and learning rate scheduler\n    criterion_val = nn.CrossEntropyLoss().to(device)\n#     criterion=torch.nn.KLDivLoss(reduction=\"sum\").to(device)\n\n    optimizer = torch.optim.Adam(student.parameters(), lr=args.lr,\n                                weight_decay=args.weight_decay)\n    \n    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n    \"\"\"scheduler = StepLR(optimizer, step_size=250, gamma=0.1)\"\"\"\n    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = args.epochs + 1, T_mult=1, eta_min=args.min_lr, verbose=False)\n    \n#     scheduler = CosineAnnealingWarmRestarts(optimizer, T_0 = 30, T_mult=1, eta_min=args.min_lr, verbose=False)\n    \n    # optionally resume from a checkpoint\n    if args.resume:\n        \n        if os.path.isfile(args.resume):\n            print(\"=> loading checkpoint '{}'\".format(args.resume))\n            if torch.cuda.is_available():\n                checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint['epoch']\n            best_acc1 = checkpoint['best_acc1']\n\n            student.load_state_dict(checkpoint['state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            scheduler.load_state_dict(checkpoint['scheduler'])\n            print(\"=> loaded checkpoint '{}' (epoch {})\"\n                  .format(args.resume, checkpoint['epoch']))\n        else:\n            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n\n\n#     train_transform = transforms.Compose(\n#     [transforms.RandomHorizontalFlip(),\n#         transforms.RandomRotation(20),transforms.ToTensor(),\n#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    \n    train_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    \n    val_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n\n   \n    train_dataset=torchvision.datasets.CIFAR100('./data', train=True, download=True,transform=train_transform)\n    \n    test_loader, val_loader = get_train_valid_loader(\"./data\",\n                            batch_size = 4,\n                            transform_list=val_transform,\n                            random_seed = 1,\n                            valid_size=0.5,\n                            shuffle=True,\n                            show_sample=False,\n                            num_workers=1,\n                            pin_memory=True)\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers)\n\n\n    \"\"\" if args.evaluate:\n        validate(val_loader, student, criterion, args)\n        return\"\"\"\n    early_stopper = EarlyStopper(patience=3, min_delta=10)\n    \n#     logs,losses = find_lr(train_loader,criterion_val)\n#     plt.plot(logs[10:-5],losses[10:-5])\n    for epoch in range(args.start_epoch, args.epochs):\n\n        # train for one epoch\n        writer = train(train_loader, student, criterion_val, optimizer, epoch, device, writer, args)\n\n        # evaluate on validation set # change this as this is the measure of whether the model is good or not versus the training/ validation set\n        acc1, writer = validate(val_loader, student, criterion_val, writer, epoch, args)\n        \n        scheduler.step()\n        \n        # remember best acc@1 and save checkpoint\n        is_best = acc1 > best_acc1\n        best_acc1 = max(acc1, best_acc1)\n\n        if early_stopper.early_stop(acc1):             \n            break\n        save_checkpoint(\n                state={'epoch': epoch + 1,\n                'student': args.student,\n                'state_dict': student.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n                'scheduler' : scheduler.state_dict()\n                }, \n                is_best=is_best,\n                args=args,\n                filename=os.path.join(\"checkpoints\",args.experiment_name +'_checkpoint.pth.tar'))\n    writer.flush()\n\ndef train(train_loader, student, criterion, optimizer, epoch, device, writer, args):\n    batch_time = AverageMeter('Time', ':6.3f')\n    # data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    top5 = AverageMeter('Acc@5', ':6.2f')\n\n#     top1_teacher = AverageMeter('Teacher Acc@1', ':6.2f')\n#     top5_teacher = AverageMeter('Teacher Acc@5', ':6.2f')\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, losses, top1, top5],\n        prefix=\"Epoch: [{}]\".format(epoch))\n\n    # switch to train mode\n    student.train()\n#     teacher.eval()\n\n    end = time.time()\n    for i, (images, target) in enumerate(train_loader):\n        # measure data loading time\n        # data_time.update(time.time() - end)\n\n        # move data to the same device as model\n        images = images.to(device)#, non_blocking=True)\n        target = target.to(device)#, non_blocking=True)\n        # writer.add_graph(student, images)\n        # writer.add_graph(teacher, images)\n        optimizer.zero_grad()\n        \n        # compute output\n        output_student_raw = student(images)\n#         output_teacher_raw = teacher(images)\n\n\n\n#         output_student = F.log_softmax(output_student_raw/args.temperature, dim=1)\n#         output_teacher = F.softmax(output_teacher_raw/args.temperature, dim=1)\n        loss = criterion(output_student_raw, target)\n\n        # measure accuracy and record loss\n        acc1, acc5 = accuracy(output_student_raw, target, topk=(1, 5))\n        #for the teacher\n#         t_acc1, t_acc5 = accuracy(output_teacher_raw, target, topk=(1, 5))\n\n        losses.update(loss.item(), images.size(0))\n        top1.update(acc1[0], images.size(0))\n        top5.update(acc5[0], images.size(0))\n\n#         top1_teacher.update(t_acc1[0], images.size(0))\n#         top5_teacher.update(t_acc5[0], images.size(0))\n        # compute gradient and do SGD step\n        \n        loss.backward()\n        optimizer.step()\n        writer.add_scalar(\"Loss/train\", loss, epoch)\n        writer.add_scalar(\"Accuracy_student/train\", acc1, epoch)\n#         writer.add_scalar(\"Accuracy_teacher/train\", t_acc1, epoch)\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            progress.display(i + 1)\n    return writer\n    \n\ndef validate(val_loader, student, criterion, writer, epoch, args):\n\n    def run_validate(loader, base_progress=0):\n        with torch.no_grad():\n            end = time.time()\n            for i, (images, target) in enumerate(loader):\n                i = base_progress + i\n                images = images.to(device, non_blocking=True)\n                target = target.to(device, non_blocking=True)\n\n                # compute output\n                output = student(images)\n                loss = criterion(output, target)\n\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, target, topk=(1, 5))\n                losses.update(loss.item(), images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % args.print_freq == 0:\n                    progress.display(i + 1)\n\n    batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)\n    losses = AverageMeter('Loss', ':.4e', Summary.NONE)\n    top1 = AverageMeter('Acc@1', ':6.2f', Summary.AVERAGE)\n    top5 = AverageMeter('Acc@5', ':6.2f', Summary.AVERAGE)\n    progress = ProgressMeter(\n        len(val_loader) + ((len(val_loader.sampler) < len(val_loader.dataset))),\n        [batch_time, losses, top1, top5],\n        prefix='Test: ')\n\n    # switch to evaluate mode\n    student.eval()\n\n    run_validate(val_loader)\n    # if args.distributed:\n    #     top1.all_reduce()\n    #     top5.all_reduce()\n\n    if (len(val_loader.sampler) < len(val_loader.dataset)):\n        aux_val_dataset = Subset(val_loader.dataset,\n                                 range(len(val_loader.sampler), len(val_loader.dataset)))\n        aux_val_loader = torch.utils.data.DataLoader(\n            aux_val_dataset, batch_size=args.batch_size, shuffle=False,\n            num_workers=args.workers, pin_memory=True)\n        run_validate(aux_val_loader, len(val_loader))\n\n    progress.display_summary()\n    \n    writer.add_scalar(\"Loss_student/Val\", losses.avg, epoch)\n    writer.add_scalar(\"Accuracy_student/Val\", top1.avg, epoch)\n\n    return top1.avg, writer\n\n\ndef save_checkpoint(state, is_best, args,filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, os.path.join('best_models',args.experiment_name+'_model_best.pth.tar'))\n\nclass Summary(Enum):\n    NONE = 0\n    AVERAGE = 1\n    SUM = 2\n    COUNT = 3\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self, name, fmt=':f', summary_type=Summary.AVERAGE):\n        self.name = name\n        self.fmt = fmt\n        self.summary_type = summary_type\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def all_reduce(self):\n        if torch.cuda.is_available():\n            device = torch.device(\"cuda\")\n        else:\n            device = torch.device(\"cpu\")\n        total = torch.tensor([self.sum, self.count], dtype=torch.float32, device=device)\n        # dist.all_reduce(total, dist.ReduceOp.SUM, async_op=False)\n        self.sum, self.count = total.tolist()\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n    \n    def summary(self):\n        fmtstr = ''\n        if self.summary_type is Summary.NONE:\n            fmtstr = ''\n        elif self.summary_type is Summary.AVERAGE:\n            fmtstr = '{name} {avg:.3f}'\n        elif self.summary_type is Summary.SUM:\n            fmtstr = '{name} {sum:.3f}'\n        elif self.summary_type is Summary.COUNT:\n            fmtstr = '{name} {count:.3f}'\n        else:\n            raise ValueError('invalid summary type %r' % self.summary_type)\n        \n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n        \n    def display_summary(self):\n        entries = [\" *\"]\n        entries += [meter.summary() for meter in self.meters]\n        print(' '.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res\ndef add_pr_curve_tensorboard(class_index, test_probs, test_label,classes, global_step=0):\n    '''\n    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n    precision-recall curve\n    '''\n    tensorboard_truth = test_label == class_index\n    tensorboard_probs = test_probs[:, class_index]\n\n    writer.add_pr_curve(classes[class_index],\n                        tensorboard_truth,\n                        tensorboard_probs,\n                        global_step=global_step)\n    writer.close()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:35:03.684282Z","iopub.execute_input":"2023-06-09T15:35:03.684619Z","iopub.status.idle":"2023-06-09T15:35:03.780388Z","shell.execute_reply.started":"2023-06-09T15:35:03.684588Z","shell.execute_reply":"2023-06-09T15:35:03.779352Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# from main_training import main\nfrom torchvision.models import mobilenet_v3_small\nstudent = mobilenet_v3_small(pretrained=False, num_classes=100)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:35:03.782805Z","iopub.execute_input":"2023-06-09T15:35:03.784642Z","iopub.status.idle":"2023-06-09T15:35:03.834713Z","shell.execute_reply.started":"2023-06-09T15:35:03.784604Z","shell.execute_reply":"2023-06-09T15:35:03.833809Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"import torch\n# from main_training import main\nfrom torchvision.models import mobilenet_v3_small\nfrom torch.optim.lr_scheduler import StepLR, CosineAnnealingWarmRestarts\nimport matplotlib.pyplot as plt\n\nclass argclass(object):\n    def __init__(self, *initial_data,**kwargs):\n        for dictionary in initial_data:\n            for key in dictionary:\n                setattr(self, key, dictionary[key])\n        for key in kwargs:\n            setattr(self, key, kwargs[key])\n\n# teacher3=torch.hub.load(\"chenyaofo/pytorch-cifar-models\",'cifar100_mobilenetv2_x1_0',pretrained=True)\n\nhyperparams={'lr':0.001,\n              'min_lr':0,\n             'momentum':0.9,\n             'weight_decay':1e-4,\n             'batch_size':128,\n             'epochs':25,\n             'start_epoch':0,\n             'print_freq':50,\n             'pretrained':True,\n             'evaluate':False,\n             'world_size':-1,\n             'rank':-1,\n             'dist_url':'tcp://',\n             'dist_backend':'nccl',\n             'seed':None,\n             'gpu':None,\n             'multiprocessing_distributed':False,\n             'dummy':False,\n             'data':'./data',\n             'workers':4,\n            #  'resume':'model_best.pth.tar',\n             'resume':'',\n             'evaluate':False,\n#              'teacher':teacher3,\n             'student':mobilenet_v3_small(pretrained=False, num_classes=100),\n             \"experiment_name\":\"test-student_from_scratch-v1\",\n#              'temperature':5\n            }\n\nargs=argclass(hyperparams)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:35:03.836110Z","iopub.execute_input":"2023-06-09T15:35:03.836991Z","iopub.status.idle":"2023-06-09T15:35:03.890944Z","shell.execute_reply.started":"2023-06-09T15:35:03.836956Z","shell.execute_reply":"2023-06-09T15:35:03.890049Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"main(args)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T15:35:03.892263Z","iopub.execute_input":"2023-06-09T15:35:03.892794Z","iopub.status.idle":"2023-06-09T15:37:15.779630Z","shell.execute_reply.started":"2023-06-09T15:35:03.892740Z","shell.execute_reply":"2023-06-09T15:37:15.777862Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Use GPU for training : CUDA version 11.8, device:cuda:0\n=> Loading Student model 'MobileNetV3'\nFiles already downloaded and verified\nFiles already downloaded and verified\nFiles already downloaded and verified\nEpoch: [0][  1/391]\tTime  0.478 ( 0.478)\tLoss 4.6026e+00 (4.6026e+00)\tAcc@1   1.56 (  1.56)\tAcc@5   6.25 (  6.25)\nEpoch: [0][ 51/391]\tTime  0.040 ( 0.054)\tLoss 4.5151e+00 (4.4991e+00)\tAcc@1   0.78 (  2.47)\tAcc@5  14.06 ( 10.68)\nEpoch: [0][101/391]\tTime  0.039 ( 0.050)\tLoss 4.1407e+00 (4.3568e+00)\tAcc@1   3.91 (  3.49)\tAcc@5  20.31 ( 14.74)\nEpoch: [0][151/391]\tTime  0.065 ( 0.050)\tLoss 4.0699e+00 (4.2576e+00)\tAcc@1   3.91 (  4.55)\tAcc@5  24.22 ( 17.62)\nEpoch: [0][201/391]\tTime  0.041 ( 0.049)\tLoss 3.9177e+00 (4.1930e+00)\tAcc@1   8.59 (  5.21)\tAcc@5  28.91 ( 19.59)\nEpoch: [0][251/391]\tTime  0.038 ( 0.048)\tLoss 3.7243e+00 (4.1393e+00)\tAcc@1  10.16 (  5.84)\tAcc@5  31.25 ( 21.18)\nEpoch: [0][301/391]\tTime  0.040 ( 0.047)\tLoss 3.9762e+00 (4.0922e+00)\tAcc@1   3.91 (  6.46)\tAcc@5  25.78 ( 22.66)\nEpoch: [0][351/391]\tTime  0.043 ( 0.047)\tLoss 3.7099e+00 (4.0497e+00)\tAcc@1  10.16 (  6.97)\tAcc@5  31.25 ( 24.01)\nTest: [   1/1251]\tTime  0.088 ( 0.088)\tLoss 4.6422e+00 (4.6422e+00)\tAcc@1   0.00 (  0.00)\tAcc@5   0.00 (  0.00)\nTest: [  51/1251]\tTime  0.010 ( 0.011)\tLoss 4.6614e+00 (4.6559e+00)\tAcc@1   0.00 (  0.98)\tAcc@5  25.00 (  4.90)\nTest: [ 101/1251]\tTime  0.009 ( 0.010)\tLoss 4.7043e+00 (4.6408e+00)\tAcc@1   0.00 (  0.99)\tAcc@5   0.00 (  4.95)\nTest: [ 151/1251]\tTime  0.009 ( 0.010)\tLoss 4.9653e+00 (4.6365e+00)\tAcc@1   0.00 (  0.66)\tAcc@5   0.00 (  4.97)\nTest: [ 201/1251]\tTime  0.009 ( 0.010)\tLoss 4.5813e+00 (4.6446e+00)\tAcc@1   0.00 (  0.50)\tAcc@5   0.00 (  4.98)\nTest: [ 251/1251]\tTime  0.009 ( 0.010)\tLoss 4.5489e+00 (4.6355e+00)\tAcc@1   0.00 (  0.50)\tAcc@5   0.00 (  4.78)\nTest: [ 301/1251]\tTime  0.009 ( 0.010)\tLoss 4.5128e+00 (4.6404e+00)\tAcc@1   0.00 (  0.42)\tAcc@5   0.00 (  4.40)\nTest: [ 351/1251]\tTime  0.009 ( 0.010)\tLoss 4.9393e+00 (4.6423e+00)\tAcc@1   0.00 (  0.50)\tAcc@5   0.00 (  4.27)\nTest: [ 401/1251]\tTime  0.010 ( 0.010)\tLoss 4.4558e+00 (4.6422e+00)\tAcc@1   0.00 (  0.62)\tAcc@5   0.00 (  4.68)\nTest: [ 451/1251]\tTime  0.009 ( 0.010)\tLoss 4.5823e+00 (4.6375e+00)\tAcc@1   0.00 (  0.78)\tAcc@5   0.00 (  4.88)\nTest: [ 501/1251]\tTime  0.010 ( 0.010)\tLoss 4.4907e+00 (4.6363e+00)\tAcc@1   0.00 (  0.80)\tAcc@5   0.00 (  5.09)\nTest: [ 551/1251]\tTime  0.009 ( 0.010)\tLoss 4.5655e+00 (4.6340e+00)\tAcc@1   0.00 (  0.73)\tAcc@5   0.00 (  5.31)\nTest: [ 601/1251]\tTime  0.009 ( 0.010)\tLoss 4.7649e+00 (4.6320e+00)\tAcc@1   0.00 (  0.67)\tAcc@5   0.00 (  5.16)\nTest: [ 651/1251]\tTime  0.009 ( 0.010)\tLoss 4.6031e+00 (4.6335e+00)\tAcc@1   0.00 (  0.65)\tAcc@5   0.00 (  5.07)\nTest: [ 701/1251]\tTime  0.010 ( 0.010)\tLoss 4.6076e+00 (4.6306e+00)\tAcc@1   0.00 (  0.71)\tAcc@5   0.00 (  5.28)\nTest: [ 751/1251]\tTime  0.010 ( 0.010)\tLoss 4.8021e+00 (4.6319e+00)\tAcc@1   0.00 (  0.70)\tAcc@5   0.00 (  5.16)\nTest: [ 801/1251]\tTime  0.016 ( 0.010)\tLoss 4.6624e+00 (4.6322e+00)\tAcc@1   0.00 (  0.69)\tAcc@5  25.00 (  5.09)\nTest: [ 851/1251]\tTime  0.012 ( 0.010)\tLoss 4.5075e+00 (4.6341e+00)\tAcc@1  25.00 (  0.79)\tAcc@5  25.00 (  5.08)\nTest: [ 901/1251]\tTime  0.009 ( 0.010)\tLoss 4.5327e+00 (4.6365e+00)\tAcc@1   0.00 (  0.86)\tAcc@5   0.00 (  5.19)\nTest: [ 951/1251]\tTime  0.009 ( 0.010)\tLoss 4.5429e+00 (4.6359e+00)\tAcc@1   0.00 (  0.87)\tAcc@5   0.00 (  5.18)\nTest: [1001/1251]\tTime  0.025 ( 0.010)\tLoss 4.4616e+00 (4.6376e+00)\tAcc@1   0.00 (  0.87)\tAcc@5   0.00 (  5.09)\nTest: [1051/1251]\tTime  0.009 ( 0.010)\tLoss 4.6869e+00 (4.6373e+00)\tAcc@1   0.00 (  0.90)\tAcc@5   0.00 (  5.11)\nTest: [1101/1251]\tTime  0.010 ( 0.010)\tLoss 4.3314e+00 (4.6373e+00)\tAcc@1   0.00 (  0.91)\tAcc@5  25.00 (  5.09)\nTest: [1151/1251]\tTime  0.010 ( 0.010)\tLoss 4.4850e+00 (4.6377e+00)\tAcc@1   0.00 (  0.96)\tAcc@5  25.00 (  5.21)\nTest: [1201/1251]\tTime  0.009 ( 0.010)\tLoss 4.5106e+00 (4.6371e+00)\tAcc@1   0.00 (  0.96)\tAcc@5   0.00 (  5.16)\nTest: [1251/1251]\tTime  0.272 ( 0.010)\tLoss 4.6378e+00 (4.6372e+00)\tAcc@1   3.12 (  1.05)\tAcc@5  10.16 (  5.40)\n *   Acc@1 1.020 Acc@5 5.230\nEpoch: [1][  1/391]\tTime  0.383 ( 0.383)\tLoss 3.8582e+00 (3.8582e+00)\tAcc@1   8.59 (  8.59)\tAcc@5  35.16 ( 35.16)\nEpoch: [1][ 51/391]\tTime  0.044 ( 0.054)\tLoss 3.5782e+00 (3.7073e+00)\tAcc@1  13.28 ( 11.80)\tAcc@5  41.41 ( 35.20)\nEpoch: [1][101/391]\tTime  0.038 ( 0.049)\tLoss 3.8270e+00 (3.6721e+00)\tAcc@1   7.81 ( 12.23)\tAcc@5  34.38 ( 36.10)\nEpoch: [1][151/391]\tTime  0.038 ( 0.050)\tLoss 3.5247e+00 (3.6470e+00)\tAcc@1  14.84 ( 12.94)\tAcc@5  39.06 ( 36.72)\nEpoch: [1][201/391]\tTime  0.045 ( 0.049)\tLoss 3.4761e+00 (3.6188e+00)\tAcc@1  11.72 ( 13.34)\tAcc@5  37.50 ( 37.56)\nEpoch: [1][251/391]\tTime  0.058 ( 0.048)\tLoss 3.4646e+00 (3.5984e+00)\tAcc@1  17.97 ( 13.85)\tAcc@5  43.75 ( 38.21)\nEpoch: [1][301/391]\tTime  0.040 ( 0.047)\tLoss 3.5614e+00 (3.5812e+00)\tAcc@1  18.75 ( 14.06)\tAcc@5  42.19 ( 38.73)\nEpoch: [1][351/391]\tTime  0.054 ( 0.047)\tLoss 3.3242e+00 (3.5661e+00)\tAcc@1  17.19 ( 14.39)\tAcc@5  46.88 ( 39.33)\nTest: [   1/1251]\tTime  0.081 ( 0.081)\tLoss 2.8762e+00 (2.8762e+00)\tAcc@1  50.00 ( 50.00)\tAcc@5  50.00 ( 50.00)\nTest: [  51/1251]\tTime  0.009 ( 0.011)\tLoss 3.6684e+00 (3.4556e+00)\tAcc@1   0.00 ( 19.12)\tAcc@5  25.00 ( 43.14)\nTest: [ 101/1251]\tTime  0.009 ( 0.010)\tLoss 4.0386e+00 (3.4920e+00)\tAcc@1  50.00 ( 17.33)\tAcc@5  50.00 ( 44.06)\nTest: [ 151/1251]\tTime  0.009 ( 0.010)\tLoss 3.6878e+00 (3.5030e+00)\tAcc@1   0.00 ( 16.56)\tAcc@5  25.00 ( 43.21)\nTest: [ 201/1251]\tTime  0.009 ( 0.010)\tLoss 3.1455e+00 (3.5110e+00)\tAcc@1   0.00 ( 15.80)\tAcc@5  50.00 ( 42.16)\nTest: [ 251/1251]\tTime  0.009 ( 0.009)\tLoss 3.7885e+00 (3.4975e+00)\tAcc@1  25.00 ( 16.04)\tAcc@5  25.00 ( 42.33)\nTest: [ 301/1251]\tTime  0.009 ( 0.009)\tLoss 3.0969e+00 (3.5320e+00)\tAcc@1  50.00 ( 15.86)\tAcc@5  75.00 ( 41.53)\nTest: [ 351/1251]\tTime  0.009 ( 0.009)\tLoss 2.7859e+00 (3.5226e+00)\tAcc@1   0.00 ( 15.88)\tAcc@5  50.00 ( 41.52)\nTest: [ 401/1251]\tTime  0.009 ( 0.009)\tLoss 3.6885e+00 (3.5006e+00)\tAcc@1   0.00 ( 16.02)\tAcc@5  50.00 ( 42.46)\nTest: [ 451/1251]\tTime  0.009 ( 0.009)\tLoss 4.1829e+00 (3.4969e+00)\tAcc@1   0.00 ( 15.91)\tAcc@5   0.00 ( 42.74)\nTest: [ 501/1251]\tTime  0.010 ( 0.009)\tLoss 3.1540e+00 (3.4863e+00)\tAcc@1  25.00 ( 15.97)\tAcc@5  50.00 ( 42.76)\nTest: [ 551/1251]\tTime  0.009 ( 0.009)\tLoss 2.9751e+00 (3.4773e+00)\tAcc@1  25.00 ( 16.24)\tAcc@5  75.00 ( 42.79)\nTest: [ 601/1251]\tTime  0.009 ( 0.009)\tLoss 3.4736e+00 (3.4756e+00)\tAcc@1  25.00 ( 16.01)\tAcc@5  50.00 ( 43.30)\nTest: [ 651/1251]\tTime  0.010 ( 0.009)\tLoss 4.4192e+00 (3.4747e+00)\tAcc@1   0.00 ( 16.01)\tAcc@5   0.00 ( 43.13)\nTest: [ 701/1251]\tTime  0.013 ( 0.010)\tLoss 3.1124e+00 (3.4734e+00)\tAcc@1  25.00 ( 16.19)\tAcc@5  50.00 ( 42.94)\nTest: [ 751/1251]\tTime  0.010 ( 0.010)\tLoss 3.1811e+00 (3.4749e+00)\tAcc@1  50.00 ( 16.18)\tAcc@5  50.00 ( 42.88)\nTest: [ 801/1251]\tTime  0.009 ( 0.010)\tLoss 3.5341e+00 (3.4687e+00)\tAcc@1  25.00 ( 16.57)\tAcc@5  50.00 ( 43.04)\nTest: [ 851/1251]\tTime  0.009 ( 0.010)\tLoss 3.9068e+00 (3.4679e+00)\tAcc@1   0.00 ( 16.95)\tAcc@5  50.00 ( 42.95)\nTest: [ 901/1251]\tTime  0.010 ( 0.010)\tLoss 2.1817e+00 (3.4634e+00)\tAcc@1  25.00 ( 16.98)\tAcc@5 100.00 ( 42.95)\nTest: [ 951/1251]\tTime  0.009 ( 0.010)\tLoss 3.1675e+00 (3.4593e+00)\tAcc@1   0.00 ( 16.98)\tAcc@5  50.00 ( 42.77)\nTest: [1001/1251]\tTime  0.009 ( 0.010)\tLoss 3.3854e+00 (3.4588e+00)\tAcc@1   0.00 ( 16.93)\tAcc@5  75.00 ( 42.98)\nTest: [1051/1251]\tTime  0.009 ( 0.010)\tLoss 3.5053e+00 (3.4542e+00)\tAcc@1   0.00 ( 17.08)\tAcc@5  25.00 ( 43.05)\nTest: [1101/1251]\tTime  0.009 ( 0.010)\tLoss 2.4197e+00 (3.4496e+00)\tAcc@1  50.00 ( 17.03)\tAcc@5  75.00 ( 43.19)\nTest: [1151/1251]\tTime  0.009 ( 0.010)\tLoss 4.2491e+00 (3.4492e+00)\tAcc@1   0.00 ( 17.05)\tAcc@5   0.00 ( 43.14)\nTest: [1201/1251]\tTime  0.009 ( 0.010)\tLoss 4.0125e+00 (3.4453e+00)\tAcc@1   0.00 ( 17.11)\tAcc@5  50.00 ( 43.26)\nTest: [1251/1251]\tTime  0.283 ( 0.010)\tLoss 3.5436e+00 (3.4515e+00)\tAcc@1  17.19 ( 16.99)\tAcc@5  42.19 ( 43.25)\n *   Acc@1 17.150 Acc@5 43.350\nEpoch: [2][  1/391]\tTime  0.402 ( 0.402)\tLoss 3.5352e+00 (3.5352e+00)\tAcc@1  17.19 ( 17.19)\tAcc@5  43.75 ( 43.75)\nEpoch: [2][ 51/391]\tTime  0.039 ( 0.054)\tLoss 3.3423e+00 (3.3605e+00)\tAcc@1  17.97 ( 17.42)\tAcc@5  40.62 ( 44.42)\nEpoch: [2][101/391]\tTime  0.042 ( 0.052)\tLoss 3.1225e+00 (3.3501e+00)\tAcc@1  15.62 ( 17.95)\tAcc@5  51.56 ( 45.01)\nEpoch: [2][151/391]\tTime  0.051 ( 0.051)\tLoss 3.4269e+00 (3.3481e+00)\tAcc@1  13.28 ( 18.08)\tAcc@5  44.53 ( 45.18)\nEpoch: [2][201/391]\tTime  0.051 ( 0.050)\tLoss 3.2218e+00 (3.3375e+00)\tAcc@1  21.09 ( 18.25)\tAcc@5  46.09 ( 45.54)\nEpoch: [2][251/391]\tTime  0.039 ( 0.048)\tLoss 3.3985e+00 (3.3232e+00)\tAcc@1  16.41 ( 18.55)\tAcc@5  45.31 ( 46.02)\nEpoch: [2][301/391]\tTime  0.041 ( 0.048)\tLoss 3.2484e+00 (3.3126e+00)\tAcc@1  18.75 ( 18.79)\tAcc@5  51.56 ( 46.35)\nEpoch: [2][351/391]\tTime  0.038 ( 0.047)\tLoss 3.1276e+00 (3.3051e+00)\tAcc@1  25.00 ( 18.97)\tAcc@5  53.12 ( 46.56)\nTest: [   1/1251]\tTime  0.081 ( 0.081)\tLoss 2.8397e+00 (2.8397e+00)\tAcc@1  50.00 ( 50.00)\tAcc@5  50.00 ( 50.00)\nTest: [  51/1251]\tTime  0.009 ( 0.011)\tLoss 2.2590e+00 (3.2752e+00)\tAcc@1  25.00 ( 20.10)\tAcc@5  50.00 ( 46.08)\nTest: [ 101/1251]\tTime  0.009 ( 0.010)\tLoss 3.2879e+00 (3.2517e+00)\tAcc@1   0.00 ( 21.78)\tAcc@5  75.00 ( 50.00)\nTest: [ 151/1251]\tTime  0.010 ( 0.010)\tLoss 3.9662e+00 (3.2483e+00)\tAcc@1   0.00 ( 21.69)\tAcc@5  25.00 ( 48.84)\nTest: [ 201/1251]\tTime  0.009 ( 0.010)\tLoss 4.2431e+00 (3.2227e+00)\tAcc@1   0.00 ( 22.14)\tAcc@5   0.00 ( 48.76)\nTest: [ 251/1251]\tTime  0.009 ( 0.009)\tLoss 3.7897e+00 (3.2468e+00)\tAcc@1   0.00 ( 21.12)\tAcc@5  50.00 ( 48.31)\nTest: [ 301/1251]\tTime  0.009 ( 0.009)\tLoss 2.3496e+00 (3.2605e+00)\tAcc@1  50.00 ( 21.43)\tAcc@5  75.00 ( 48.26)\nTest: [ 351/1251]\tTime  0.009 ( 0.009)\tLoss 3.5601e+00 (3.2745e+00)\tAcc@1   0.00 ( 21.01)\tAcc@5  50.00 ( 48.22)\nTest: [ 401/1251]\tTime  0.009 ( 0.009)\tLoss 3.6696e+00 (3.2898e+00)\tAcc@1  25.00 ( 20.70)\tAcc@5  50.00 ( 47.63)\nTest: [ 451/1251]\tTime  0.009 ( 0.009)\tLoss 1.7575e+00 (3.3033e+00)\tAcc@1  50.00 ( 20.51)\tAcc@5  75.00 ( 47.34)\nTest: [ 501/1251]\tTime  0.009 ( 0.009)\tLoss 2.5450e+00 (3.3052e+00)\tAcc@1  50.00 ( 20.61)\tAcc@5  75.00 ( 47.90)\nTest: [ 551/1251]\tTime  0.009 ( 0.009)\tLoss 2.4661e+00 (3.2942e+00)\tAcc@1   0.00 ( 20.74)\tAcc@5  75.00 ( 48.28)\nTest: [ 601/1251]\tTime  0.011 ( 0.010)\tLoss 2.2337e+00 (3.3028e+00)\tAcc@1  25.00 ( 20.51)\tAcc@5  75.00 ( 47.92)\nTest: [ 651/1251]\tTime  0.012 ( 0.010)\tLoss 3.6372e+00 (3.3015e+00)\tAcc@1   0.00 ( 20.85)\tAcc@5  25.00 ( 48.20)\nTest: [ 701/1251]\tTime  0.012 ( 0.010)\tLoss 2.0793e+00 (3.3103e+00)\tAcc@1  50.00 ( 20.68)\tAcc@5  75.00 ( 47.86)\nTest: [ 751/1251]\tTime  0.010 ( 0.010)\tLoss 3.4542e+00 (3.3063e+00)\tAcc@1  25.00 ( 20.57)\tAcc@5  50.00 ( 48.04)\nTest: [ 801/1251]\tTime  0.009 ( 0.010)\tLoss 3.4126e+00 (3.2937e+00)\tAcc@1   0.00 ( 20.66)\tAcc@5  50.00 ( 48.35)\nTest: [ 851/1251]\tTime  0.009 ( 0.010)\tLoss 3.7108e+00 (3.2907e+00)\tAcc@1   0.00 ( 20.56)\tAcc@5  25.00 ( 48.09)\nTest: [ 901/1251]\tTime  0.009 ( 0.010)\tLoss 2.0384e+00 (3.2935e+00)\tAcc@1  75.00 ( 20.73)\tAcc@5 100.00 ( 48.20)\nTest: [ 951/1251]\tTime  0.009 ( 0.010)\tLoss 3.1775e+00 (3.2910e+00)\tAcc@1  25.00 ( 20.66)\tAcc@5  50.00 ( 48.29)\nTest: [1001/1251]\tTime  0.009 ( 0.010)\tLoss 2.6331e+00 (3.2876e+00)\tAcc@1  25.00 ( 20.80)\tAcc@5  75.00 ( 48.35)\nTest: [1051/1251]\tTime  0.009 ( 0.010)\tLoss 3.1220e+00 (3.2789e+00)\tAcc@1   0.00 ( 20.98)\tAcc@5  50.00 ( 48.57)\nTest: [1101/1251]\tTime  0.009 ( 0.010)\tLoss 2.5466e+00 (3.2797e+00)\tAcc@1  25.00 ( 20.82)\tAcc@5 100.00 ( 48.61)\nTest: [1151/1251]\tTime  0.009 ( 0.010)\tLoss 2.7186e+00 (3.2748e+00)\tAcc@1  25.00 ( 20.87)\tAcc@5  50.00 ( 48.57)\nTest: [1201/1251]\tTime  0.010 ( 0.010)\tLoss 3.1876e+00 (3.2805e+00)\tAcc@1  25.00 ( 20.80)\tAcc@5  50.00 ( 48.44)\nTest: [1251/1251]\tTime  0.261 ( 0.010)\tLoss 3.3210e+00 (3.2716e+00)\tAcc@1  21.88 ( 21.00)\tAcc@5  46.09 ( 48.63)\n *   Acc@1 20.910 Acc@5 48.760\nEpoch: [3][  1/391]\tTime  0.402 ( 0.402)\tLoss 2.7660e+00 (2.7660e+00)\tAcc@1  29.69 ( 29.69)\tAcc@5  61.72 ( 61.72)\nEpoch: [3][ 51/391]\tTime  0.043 ( 0.056)\tLoss 3.4630e+00 (3.1030e+00)\tAcc@1  18.75 ( 23.56)\tAcc@5  49.22 ( 51.99)\nEpoch: [3][101/391]\tTime  0.039 ( 0.050)\tLoss 2.9411e+00 (3.1147e+00)\tAcc@1  24.22 ( 23.02)\tAcc@5  55.47 ( 51.60)\nEpoch: [3][151/391]\tTime  0.048 ( 0.049)\tLoss 2.9127e+00 (3.1188e+00)\tAcc@1  24.22 ( 22.72)\tAcc@5  57.81 ( 51.61)\nEpoch: [3][201/391]\tTime  0.036 ( 0.048)\tLoss 3.4452e+00 (3.1132e+00)\tAcc@1  17.97 ( 22.66)\tAcc@5  41.41 ( 51.68)\nEpoch: [3][251/391]\tTime  0.052 ( 0.047)\tLoss 3.2220e+00 (3.1171e+00)\tAcc@1  23.44 ( 22.74)\tAcc@5  49.22 ( 51.62)\nEpoch: [3][301/391]\tTime  0.045 ( 0.047)\tLoss 3.1908e+00 (3.1165e+00)\tAcc@1  21.88 ( 22.68)\tAcc@5  47.66 ( 51.67)\nEpoch: [3][351/391]\tTime  0.046 ( 0.046)\tLoss 3.1673e+00 (3.1134e+00)\tAcc@1  25.78 ( 22.73)\tAcc@5  54.69 ( 51.82)\nTest: [   1/1251]\tTime  0.095 ( 0.095)\tLoss 4.0899e+00 (4.0899e+00)\tAcc@1   0.00 (  0.00)\tAcc@5  25.00 ( 25.00)\nTest: [  51/1251]\tTime  0.009 ( 0.012)\tLoss 3.1423e+00 (3.1263e+00)\tAcc@1  25.00 ( 24.02)\tAcc@5  50.00 ( 51.96)\nTest: [ 101/1251]\tTime  0.009 ( 0.011)\tLoss 3.6427e+00 (3.2709e+00)\tAcc@1   0.00 ( 22.52)\tAcc@5  50.00 ( 49.75)\nTest: [ 151/1251]\tTime  0.009 ( 0.010)\tLoss 2.1534e+00 (3.1682e+00)\tAcc@1  50.00 ( 23.84)\tAcc@5  75.00 ( 51.66)\nTest: [ 201/1251]\tTime  0.009 ( 0.010)\tLoss 1.7534e+00 (3.1431e+00)\tAcc@1  75.00 ( 24.88)\tAcc@5 100.00 ( 52.74)\nTest: [ 251/1251]\tTime  0.010 ( 0.010)\tLoss 2.2465e+00 (3.1410e+00)\tAcc@1  25.00 ( 24.90)\tAcc@5  75.00 ( 52.49)\nTest: [ 301/1251]\tTime  0.009 ( 0.010)\tLoss 1.4136e+00 (3.1238e+00)\tAcc@1  50.00 ( 24.92)\tAcc@5  75.00 ( 52.66)\nTest: [ 351/1251]\tTime  0.007 ( 0.010)\tLoss 2.6164e+00 (3.1080e+00)\tAcc@1  25.00 ( 24.29)\tAcc@5  75.00 ( 53.06)\nTest: [ 401/1251]\tTime  0.009 ( 0.010)\tLoss 3.6812e+00 (3.1144e+00)\tAcc@1  25.00 ( 23.63)\tAcc@5  25.00 ( 52.74)\nTest: [ 451/1251]\tTime  0.009 ( 0.010)\tLoss 2.9724e+00 (3.1141e+00)\tAcc@1  25.00 ( 23.56)\tAcc@5  50.00 ( 52.72)\nTest: [ 501/1251]\tTime  0.009 ( 0.010)\tLoss 3.5665e+00 (3.1289e+00)\tAcc@1   0.00 ( 23.20)\tAcc@5  25.00 ( 52.35)\nTest: [ 551/1251]\tTime  0.012 ( 0.010)\tLoss 2.7889e+00 (3.1242e+00)\tAcc@1   0.00 ( 23.19)\tAcc@5  75.00 ( 52.63)\nTest: [ 601/1251]\tTime  0.012 ( 0.010)\tLoss 4.1948e+00 (3.1297e+00)\tAcc@1   0.00 ( 23.34)\tAcc@5  25.00 ( 52.37)\nTest: [ 651/1251]\tTime  0.009 ( 0.010)\tLoss 3.5201e+00 (3.1245e+00)\tAcc@1  25.00 ( 23.12)\tAcc@5  25.00 ( 52.19)\nTest: [ 701/1251]\tTime  0.009 ( 0.010)\tLoss 2.1013e+00 (3.1286e+00)\tAcc@1  50.00 ( 22.90)\tAcc@5  75.00 ( 51.78)\nTest: [ 751/1251]\tTime  0.010 ( 0.010)\tLoss 5.4170e+00 (3.1306e+00)\tAcc@1   0.00 ( 22.94)\tAcc@5   0.00 ( 51.66)\nTest: [ 801/1251]\tTime  0.009 ( 0.010)\tLoss 1.2325e+00 (3.1307e+00)\tAcc@1  50.00 ( 22.97)\tAcc@5 100.00 ( 51.90)\nTest: [ 851/1251]\tTime  0.009 ( 0.010)\tLoss 3.8354e+00 (3.1315e+00)\tAcc@1  25.00 ( 23.00)\tAcc@5  25.00 ( 51.97)\nTest: [ 901/1251]\tTime  0.009 ( 0.010)\tLoss 3.2135e+00 (3.1243e+00)\tAcc@1  25.00 ( 23.14)\tAcc@5  50.00 ( 52.25)\nTest: [ 951/1251]\tTime  0.009 ( 0.010)\tLoss 3.4010e+00 (3.1236e+00)\tAcc@1   0.00 ( 23.16)\tAcc@5  75.00 ( 52.26)\nTest: [1001/1251]\tTime  0.010 ( 0.010)\tLoss 2.6956e+00 (3.1250e+00)\tAcc@1  25.00 ( 23.25)\tAcc@5  50.00 ( 52.17)\nTest: [1051/1251]\tTime  0.010 ( 0.010)\tLoss 4.4838e+00 (3.1310e+00)\tAcc@1   0.00 ( 23.10)\tAcc@5  25.00 ( 52.09)\nTest: [1101/1251]\tTime  0.009 ( 0.010)\tLoss 3.5987e+00 (3.1302e+00)\tAcc@1  25.00 ( 23.21)\tAcc@5  50.00 ( 51.98)\nTest: [1151/1251]\tTime  0.009 ( 0.010)\tLoss 3.7313e+00 (3.1258e+00)\tAcc@1  25.00 ( 23.37)\tAcc@5  25.00 ( 51.95)\nTest: [1201/1251]\tTime  0.009 ( 0.010)\tLoss 2.7456e+00 (3.1282e+00)\tAcc@1  50.00 ( 23.38)\tAcc@5  50.00 ( 51.92)\nTest: [1251/1251]\tTime  0.353 ( 0.010)\tLoss 3.1612e+00 (3.1300e+00)\tAcc@1  22.66 ( 23.42)\tAcc@5  47.66 ( 51.93)\n *   Acc@1 23.630 Acc@5 52.140\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}